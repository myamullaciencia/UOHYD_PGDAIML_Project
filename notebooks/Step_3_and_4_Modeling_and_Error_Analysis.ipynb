{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Step_3_and_4_Modeling_and_Error_Analysis.ipynb","private_outputs":true,"provenance":[],"machine_shape":"hm","collapsed_sections":["gJFpJEk1KUbF","fKFsVPDMCMCu","JJf6HY1gYiZr","2FaNtA4oIFyg","x8j4UOhqKsUp","xFy9tybZCQx2","9vda_05tNT12","xRLL2kB55Swh","_cPEYyzKyGnz"],"mount_file_id":"1eg9jtxWBGD8tRWIkNv0n687Vg4rUcBrR","authorship_tag":"ABX9TyOcNsy8gxfjAHU7GEK1sYvc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["### 1. Library and Data set ups"],"metadata":{"id":"gJFpJEk1KUbF"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"mOQSfu_AlEPL"},"outputs":[],"source":["!pip install yellowbrick\n","!pip install xgboost==1.4\n","import pandas as pd\n","import numpy as np\n","\n","from sklearn.model_selection import train_test_split,cross_validate,cross_val_score,GridSearchCV\n","from sklearn.preprocessing import StandardScaler\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.svm import SVC\n","from sklearn.metrics import confusion_matrix,precision_recall_curve,roc_auc_score\n","from sklearn.metrics import average_precision_score\n","\n","import xgboost as xgb\n","import pickle\n","\n","from imblearn.pipeline import make_pipeline\n","from imblearn.under_sampling import (\n","    RandomUnderSampler,\n","    AllKNN,\n","    TomekLinks,\n","    NearMiss\n",")\n","from imblearn.over_sampling import RandomOverSampler, SMOTE\n","from yellowbrick.classifier import ClassificationReport\n","from yellowbrick.classifier import ROCAUC,confusion_matrix"]},{"cell_type":"code","source":["# copying data\n","!cp  /content/drive/MyDrive/PG_Diploma_AI_ML_2021_UOHYD/PGDAIML_Project_Spam_Clustering/datos/my_final_messages_dt.csv ."],"metadata":{"id":"5eYj0-b7duuc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Importing data\n","messages_df = pd.read_csv('my_final_messages_dt.csv')"],"metadata":{"id":"zAgpSKYXd0Lf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# glance at data\n","messages_df.head()"],"metadata":{"id":"yBa3H1NVeJqM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# totals per spam class\n","messages_df.value_counts('message_flag')"],"metadata":{"id":"LBT7g5tCeyL6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Features X and Y \n","X_message_preview = messages_df['message_preview']\n","Y_message_flag = messages_df['message_flag']"],"metadata":{"id":"eyZN-kZGfN6A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# replace few un necessary stuffs\n","messages_df['message_flag'] = messages_df['message_flag'].replace(np.nan,0.0)"],"metadata":{"id":"lMo_BFsilC7K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 2. Text Features and Data Splitting"],"metadata":{"id":"fKFsVPDMCMCu"}},{"cell_type":"code","source":["# Data splitting \n","X_train_message_prev,X_test_message_prev,Y_train_message_flag,Y_test_message_flag = train_test_split(X_message_preview,\n","                                                                                                     Y_message_flag,random_state=2021,\n","                                                                                                     stratify=Y_message_flag,test_size=0.25)"],"metadata":{"id":"uXyvb5_LgeE7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f'1.X_Training set shape is {X_train_message_prev.shape}\\n2.X_Testing set shape is {X_test_message_prev.shape}\\n3.Y_Training message_flag set shape {Y_train_message_flag.shape}\\n4.Y_Testing message_flag set shape {Y_test_message_flag.shape}')\n"],"metadata":{"id":"3nIqL8MulvuZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define text tfidf vectorizer\n","my_text_vectorizer = TfidfVectorizer(ngram_range=(1,2),max_features=None,min_df=0.01)"],"metadata":{"id":"ycsttAtoqvd6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# fitting the text vectorizer on training set\n","my_text_vec_fit = my_text_vectorizer.fit(X_train_message_prev)"],"metadata":{"id":"oWLNM_mWrn6O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# creating training and testing vectors\n","X_train_msg_vec = my_text_vec_fit.transform(X_train_message_prev)\n","X_test_msg_vec= my_text_vec_fit.transform(X_test_message_prev)"],"metadata":{"id":"bFyjikmHsmb7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Standardization initializer\n","my_txt_scaler = StandardScaler()"],"metadata":{"id":"2fLdpv9U1mLo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# train and test standardized vectors\n","X_train_msg_vec_std = my_txt_scaler.fit_transform(X_train_msg_vec.toarray())\n","X_test_msg_vec_std = my_txt_scaler.fit_transform(X_test_msg_vec.toarray())"],"metadata":{"id":"0_TnVEMm1o8-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 3. Custom Functions to build models on classification algorithms"],"metadata":{"id":"JJf6HY1gYiZr"}},{"cell_type":"code","source":["def my_cf_naive_bayes(X_train,y_train,X_test,y_test,report=False):\n","    \"\"\" Naive bayes algorithm\"\"\"\n","    model_nb = GaussianNB()\n","    if report is False:\n","        model_nb.fit(X_train,y_train)\n","        print(f'----- Model on Train data -----')\n","        y_pred = model_nb.predict_proba(X_train)\n","        print(f'Train ROC_AUC Score : {roc_auc_score(y_train,y_pred[:,1])}')\n","        print(f'----- Model on Test data -----')\n","        yt_pred = model_nb.predict_proba(X_test)\n","        print(f'Test ROC_AUC Score : {roc_auc_score(y_test,yt_pred[:,1])}')\n","    else:\n","        visualizer = ROCAUC(model_nb, classes=[\"Legitimate\", \"Spam\"])\n","        visualizer.fit(X_train, y_train)\n","        visualizer.score(X_test, y_test)\n","        visualizer.show()\n","        print(f'--Conf Matrix')\n","        confusion_matrix(\n","                        model_nb,\n","                        X_train, y_train, X_test, y_test,\n","                        classes=['Legitimate', 'Spam'])"],"metadata":{"id":"bboyGJlrGtYC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def my_cf_logisticregression(X_train,y_train,X_test,y_test,report=False):\n","    \"\"\"\" Logistic Regression\"\"\"\n","    model_lr = LogisticRegression()\n","    if report is False:\n","        model_lr.fit(X_train,y_train)\n","        print(f'----- Model on Train data -----')\n","        y_pred = model_lr.predict_proba(X_train)\n","        print(f'Train ROC_AUC Score : {roc_auc_score(y_train,y_pred[:,1])}')\n","        print(f'----- Model on Test data -----')\n","        yt_pred = model_lr.predict_proba(X_test)\n","        print(f'Test ROC_AUC Score : {roc_auc_score(y_test,yt_pred[:,1])}')\n","    else:\n","        visualizer = ROCAUC(model_lr, classes=[\"Legitimate\", \"Spam\"])\n","        visualizer.fit(X_train, y_train)\n","        visualizer.score(X_test, y_test)\n","        visualizer.show()\n","        print(f'--Conf Matrix')\n","        confusion_matrix(\n","                        model_lr,\n","                        X_train, y_train, X_test, y_test,\n","                        classes=['Legitimate', 'Spam'])"],"metadata":{"id":"-JXglrNoend_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def my_cf_rf(X_train,y_train,X_test,y_test,report):\n","    \"\"\"\"Random forest\"\"\"\n","    model_rf = RandomForestClassifier(random_state=2022)\n","\n","    if report is False:\n","        model_rf.fit(X_train,y_train)\n","        print(f'----- Model on Train data -----')\n","        y_pred = model_rf.predict_proba(X_train)\n","        print(f'Train ROC_AUC Score : {roc_auc_score(y_train,y_pred[:,1])}')\n","        print(f'----- Model on Test data -----')\n","        yt_pred = model_rf.predict_proba(X_test)\n","        print(f'Test ROC_AUC Score : {roc_auc_score(y_test,yt_pred[:,1])}')\n","    else:\n","        visualizer = ROCAUC(model_rf, classes=[\"Legitimate\", \"Spam\"])\n","        visualizer.fit(X_train, y_train)\n","        visualizer.score(X_test, y_test)\n","        visualizer.show()\n","        print(f'--Conf Matrix')\n","        confusion_matrix(\n","                        model_rf,\n","                        X_train, y_train, X_test, y_test,\n","                        classes=['Legitimate', 'Spam'])"],"metadata":{"id":"U2uYU9mREdAW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def my_cf_xgb(X_train,y_train,X_test,y_test,report):\n","    \"\"\" XGB \"\"\"\n","    model_xgb = xgb.XGBClassifier(random_state=2022)\n","    \n","    if report is False:\n","        model_xgb.fit(X_train,y_train,early_stopping_rounds=10,eval_set=[(X_test,y_test)])\n","        print(f'----- Model on Train data -----')\n","        y_pred = model_xgb.predict_proba(X_train)\n","        print(f'Train ROC_AUC Score : {roc_auc_score(y_train,y_pred[:,1])}')\n","        print(f'----- Model on Test data -----')\n","        yt_pred = model_xgb.predict_proba(X_test)\n","        print(f'Test ROC_AUC Score : {roc_auc_score(y_test,yt_pred[:,1])}')\n","    else:\n","        visualizer = ROCAUC(model_xgb, classes=[\"Legitimate\", \"Spam\"])\n","        visualizer.fit(X_train, y_train)\n","        visualizer.score(X_test, y_test)\n","        visualizer.show()\n","        print(f'--Conf Matrix')\n","        confusion_matrix(\n","                        model_xgb,\n","                        X_train, y_train, X_test, y_test,\n","                        classes=['Legitimate', 'Spam'])"],"metadata":{"id":"tAFx_tTrCUsa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Classification Tech : SGD Classifier from Scratch"],"metadata":{"id":"2FaNtA4oIFyg"}},{"cell_type":"code","source":["# Initialize weights\n","def initialize_weights(dim):\n","    ''' In this function, we will initialize our weights and bias'''\n","    d = dim.shape[0]\n","    w=np.full((d),0)\n","    b=0.0\n","    return w,b"],"metadata":{"id":"AZnvxRA9IIxm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Compute sigmoids\n","def sigmoid(z):\n","    ''' In this function, we will return sigmoid of z'''\n","    # compute sigmoid(z) and return\n","    sigmoid= 1.0/(1 + np.exp(-z))\n","    return sigmoid"],"metadata":{"id":"FWkhy2W9IQ9B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# log loss \n","def logloss(y_true,y_pred):\n","    '''In this function, we will compute log loss '''\n","    log_loss = -1 * np.mean(y_true*(np.log10(y_pred)) + (1-y_true)*np.log10(1-y_pred))\n","    return log_loss"],"metadata":{"id":"gfbfJTeYIba6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Compute gradients\n","def gradient_dw(x,y,w,b,alpha,N):\n","    '''In this function, we will compute the gardient w.r.to w '''\n","    gdb = y - sigmoid(np.dot(w,x.T)+b)\n","    x_gdb = np.dot(gdb,x)\n","    a_n =(alpha/N)*w\n","    return x_gdb - a_n"],"metadata":{"id":"A8zZmXDnIdPv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Compute gradients\n","def gradient_db(x,y,w,b):\n","     '''In this function, we will compute gradient w.r.to b '''\n","     z = np.dot(w,x.T) + b\n","     zs= sigmoid(z)\n","     db = y - zs\n","     return db\n","    "],"metadata":{"id":"X_nQKYH7Iq6-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train(X_train,y_train,X_test,y_test,epochs,alpha,eta0):\n","    ''' logistic regression'''\n","    train_loss=[]\n","    test_loss=[]\n","    train_loss_avg =[]\n","    test_loss_avg = []\n","    w,b = initialize_weights(X_train[0])\n","    num_epochs=0\n","\n","    while num_epochs <epochs:\n","        num_epochs+=1\n","        for x,y,xt,yt in zip(X_train,y_train,X_test,y_test):\n","            dw = gradient_dw(x,y,w,b,alpha,X_train.shape[0])\n","            db = gradient_db(x,y,w,b)\n","            #Train preds\n","            y_pred = sigmoid(np.dot(w,x.T)+b)\n","            t_loss = logloss(y, y_pred)\n","            train_loss.append(t_loss)\n","            w = w + eta0 * dw\n","            b = np.mean(b + eta0 * db)\n","            yt_pred = sigmoid(np.dot(w,xt.T)+b)\n","            te_loss = logloss(yt, yt_pred)\n","            test_loss.append(te_loss)\n","        print(f'num_epochs {num_epochs} and avg train loss {np.mean(train_loss)} and avg test loss {np.mean(test_loss)}')\n","        train_loss_avg.append(np.mean(train_loss))\n","        test_loss_avg.append(np.mean(test_loss))\n","        train_loss.clear()\n","        test_loss.clear()\n"," \n","    return w,b,train_loss_avg,test_loss_avg"],"metadata":{"id":"HHBTZVajI1q0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Parameters\n","alpha=0.0001\n","eta0=0.0001\n","N=len(X_train_msg_vec_std)\n","epochs= 20"],"metadata":{"id":"30DrkL90JUoo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["w,b,train_loss,test_loss = train(X_train_msg_vec_std, Y_train_message_flag,X_test_msg_vec_std,Y_test_message_flag,epochs,alpha,eta0)"],"metadata":{"id":"-0be45IpJbRC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["SGD_train_test_loss_df = pd.DataFrame({'train_loss':train_loss,'test_loss':test_loss}\n","                                      )"],"metadata":{"id":"1V9iNvToLpEa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def pred(w,b, X):\n","    N = len(X)\n","    predict = []\n","    for i in range(N):\n","        z=np.dot(w,X[i])+b\n","        if sigmoid(z) >= 0.78: # sigmoid(w,x,b) returns 1/(1+exp(-(dot(x,w)+b)))\n","            predict.append(1)\n","        else:\n","            predict.append(0)\n","    return np.array(predict)\n","print(np.round(1-np.sum(Y_train_message_flag - pred(w,b,X_train_msg_vec_std))/len(X_train_msg_vec_std),4))\n","print(np.round(1-np.sum(Y_test_message_flag  - pred(w,b,X_test_msg_vec_std))/len(X_test_msg_vec_std),4))"],"metadata":{"id":"tXxUC8G9L69E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"XnyZEHjI42Rx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 4. Base Approaches - Modeling Techniques"],"metadata":{"id":"x8j4UOhqKsUp"}},{"cell_type":"code","source":["my_cf_naive_bayes(X_train_msg_vec_std, Y_train_message_flag,X_test_msg_vec_std,Y_test_message_flag,report=True)\n"],"metadata":{"id":"MaGJVyRGJdBZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["my_cf_logisticregression(X_train_msg_vec_std, Y_train_message_flag,X_test_msg_vec_std,Y_test_message_flag,report=True)"],"metadata":{"id":"fq9eO2lqf8ew"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["my_cf_xgb(X_train_msg_vec_std, Y_train_message_flag,X_test_msg_vec_std,Y_test_message_flag,report=True)"],"metadata":{"id":"TFqbvZp9CM4_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","### 5. Class Imbalance - Random Under and Over Sampling Techniques"],"metadata":{"id":"xFy9tybZCQx2"}},{"cell_type":"markdown","source":["#### Sampling techniques definations"],"metadata":{"id":"ToF5KcglK06d"}},{"cell_type":"code","source":["my_samplers_under_over_dict = {\n","\n","    'random_under': RandomUnderSampler(\n","        sampling_strategy='auto',\n","        random_state=0,\n","        replacement=False),\n","    'random_over': RandomOverSampler(\n","        sampling_strategy='auto',\n","        random_state=0)\n","}"],"metadata":{"id":"vE6yjtzoAZmK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["my_under_sampler_dict = {\n","\n","    'random': RandomUnderSampler(\n","        sampling_strategy='auto',\n","        random_state=0,\n","        replacement=False),\n","\n","    'tomek': TomekLinks(\n","        sampling_strategy='auto',\n","        n_jobs=20),\n","\n","    'allknn': AllKNN(\n","        sampling_strategy='auto',\n","        n_neighbors=8,\n","        kind_sel='all',\n","        n_jobs=20),\n","    \n","    'nm1': NearMiss(\n","        sampling_strategy='auto',\n","        version=1,\n","        n_neighbors=8,\n","        n_jobs=20)\n","}\n"],"metadata":{"id":"s7xoManobzIj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["my_over_sampler_dict = {\n","\n","    'rand_over' : RandomOverSampler(\n","        sampling_strategy='auto',\n","        random_state=0),\n","\n","    'smote' : SMOTE(\n","        sampling_strategy='auto',  # samples only the minority class\n","        random_state=0,  # for reproducibility\n","        k_neighbors=8,\n","        n_jobs=20)\n","\n","}"],"metadata":{"id":"UNFJ875EBIuQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Custom functions to make a pipeline to see which algorithms are performed better over different sampling technques"],"metadata":{"id":"_sHBeX_IK7td"}},{"cell_type":"code","source":["def my_cf_make_model(x_train,y_train,sampler):\n","\n","    model_nb = LogisticRegression()\n","\n","    pipe_de_model = make_pipeline(\n","        sampler,\n","        model_nb\n","\n","    )\n","    resultados_cv = cross_validate(\n","        pipe_de_model,\n","        x_train,\n","        y_train,\n","        scoring=\"roc_auc\",\n","        cv=2\n","    )\n","\n","    return resultados_cv['test_score'].mean(), resultados_cv['test_score'].std()\n"],"metadata":{"id":"vZD2IM_Pciok"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def my_cf_imbalance_sample_selections(x_train,y_train,sampler):\n","    models = {\n","        'logistic':LogisticRegression(),\n","        'naivebayes': GaussianNB(),\n","        'xgb': xgb.XGBClassifier(random_state=2022)\n","    }\n","\n","    for mname,mparam in models.items():\n","        pipe_de_model = make_pipeline(\n","            sampler,\n","            mparam\n","        )\n","        resultados_cv = cross_validate(\n","            pipe_de_model,\n","            x_train,\n","            y_train,\n","            scoring=\"roc_auc\",\n","            cv=3\n","        )\n","        mean_score = resultados_cv['test_score'].mean()\n","        print(f'model : {mname} : performaces:{ mean_score} ')"],"metadata":{"id":"5cvxy7MQFejO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for key,sampler in my_samplers_under_over_dict.items():\n","    print(f'Sampling Technique: {key}')\n","    my_cf_imbalance_sample_selections(X_train_msg_vec_std, Y_train_message_flag,sampler)"],"metadata":{"id":"3yxzHuFufVLN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 6.Model Selection and Hyper parameter tuning"],"metadata":{"id":"9vda_05tNT12"}},{"cell_type":"code","source":["my_RUS_spec = RandomOverSampler(random_state=2020)\n","X_msgprev_res,Y_msg_flag_res =my_RUS_spec.fit_resample(X_train_msg_vec_std,Y_train_message_flag)"],"metadata":{"id":"nfHvwOPaNZBV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["modelo_gbm = xgb.XGBClassifier(random_state=2021)"],"metadata":{"id":"r-o7zKg0ih82"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["param_grid = [\n","    {'n_estimators':[900,1000,1500,2000],\n","    'max_depth':[5,7,10],\n","    'learning_rate':[0.1,0.3,0.5,0.8],\n","     'booster':['dart','gbtree'],\n","     'gamma':[0.1,0.3,0.5],\n","     'subsample':[0.5,0.9],\n","     'colsample_bytree':[0.5,0.9],\n","     'colsample_bylevel':[0.5,0.9],\n","     'colsample_bynode':[0.5,0.9],\n","     'reg_lambda':[1,10,20]}\n","]"],"metadata":{"id":"NA7WppJpSxW2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# set up the search\n","busqueda = GridSearchCV(modelo_gbm, param_grid,scoring='roc_auc', cv=3, refit=True )\n","\n","# find best hyperparameters\n","busqueda.fit(X_train_msg_vec_std, Y_train_message_flag)"],"metadata":{"id":"WbcFqf7sSJA8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def my_cf_hyp_xgb(X_train,y_train,X_test,y_test):\n","    model_xgb = xgb.XGBClassifier(n_estimators=2500,max_depth=7,learning_rate=0.898568,booster='dart',gamma=0.010000,subsample=0.614947,\n","     colsample_bytree=0.5,\n","     colsample_bylevel=0.5,\n","     colsample_bynode=0.5,\n","     reg_lambda=10)\n","    model_xgb.fit(X_train,y_train,early_stopping_rounds=40,eval_set=[(X_test,y_test)])\n","    print(f'----- Model on Train data -----')\n","    y_pred = model_xgb.predict_proba(X_train)\n","    print(f'Train ROC_AUC Score : {roc_auc_score(y_train,y_pred[:,1])}')\n","    #print(f'----- Model on Test data -----')\n","    #yt_pred = model_xgb.predict_proba(X_test)\n","    #print(f'Test ROC_AUC Score : {roc_auc_score(y_test,yt_pred[:,1])}')"],"metadata":{"id":"RbUcNPVDPBxX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["my_cf_hyp_xgb(X_msgprev_res,Y_msg_flag_res,X_test_msg_vec_std,Y_test_message_flag)"],"metadata":{"id":"tXehtMotPrTu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_xgb = xgb.XGBClassifier(objective='binary:logistic',\n","                              n_estimators=1000,\n","                              eval_metric='auc',\n","                              use_label_encoder=False,\n","                              max_depth=7,\n","                              learning_rate=0.898568,\n","                              booster='dart',\n","                              gamma=0.010000,\n","                              subsample=0.614947,\n","                              colsample_bytree=0.5,\n","                              colsample_bylevel=0.5,\n","                              colsample_bynode=0.5,\n","                              reg_lambda=10)"],"metadata":{"id":"PNPzFluxzJpg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_xgb.fit(X_train_msg_vec_std,\n","              np.array(Y_train_message_flag),\n","              early_stopping_rounds=2,\n","              eval_set=[(X_test_msg_vec_std,np.array(Y_test_message_flag))])"],"metadata":{"id":"W9oDl0C0zPOK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 7. LSH from Scratch"],"metadata":{"id":"xRLL2kB55Swh"}},{"cell_type":"code","source":["def generate_hyperplanes(n,tot):\n","    \"\"\" Custom function to generate required hyperplanes \"\"\"\n","    np.random.seed(0)\n","    hyper_array=[]\n","    for _ in range(0,n):\n","        hyper_array.append(np.random.normal(0,1,tot))\n","    return np.array(hyper_array)"],"metadata":{"id":"tV5WERFa5VEX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Creating five hyperplanes\n","hypers = generate_hyperplanes(5,X_train_msg_vec_std.shape[1])"],"metadata":{"id":"ZtCh9uI05ZZp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def wt_trans_x(sparse_mat,hyper_array):\n","    \"\"\" Custom function to caluclate W_Trans_X \"\"\"\n","    trans_list = list()\n","    for fet in sparse_mat:\n","        #trans_list.append(fet.dot(hyper_array.T))\n","        trans_list.append(np.dot(fet,hyper_array.T))\n","    return trans_list"],"metadata":{"id":"6bD-tNSO5fUr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def wt_trans_x_np(sparse_mat,hyper_array):\n","    return sparse_mat.dot(hyper_array.T)"],"metadata":{"id":"C6j88PsY-kgy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def hash_key(vector):\n","    \"\"\"Generate a hashkey tupple with 1's and 0's\"\"\"\n","    key = tuple(map(lambda x: 1 if x>0 else 0,vector))\n","    return key"],"metadata":{"id":"Iuug4xh65iBb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create_hash_key(vec):\n","    \"\"\"create WtansX and generate hashkey on it\"\"\"\n","    wt_x_vec= wt_trans_x(vec,hypers)\n","    hk = hash_key(wt_x_vec)\n","    return hk"],"metadata":{"id":"h5-B2ktn5qfy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create_hash_table(arr):\n","    \"\"\"Generate a hashtable\"\"\"\n","    my_hash_table=dict()\n","    for idx,vec in enumerate(arr):\n","        key_gen=hash_key(vec)\n","        if key_gen not in my_hash_table.keys():\n","            my_hash_table[key_gen]=0\n","        my_hash_table[key_gen]=[]\n","    \n","    for idx,vec in enumerate(arr):\n","        key_gen=hash_key(vec)\n","        if key_gen in my_hash_table.keys():\n","            my_hash_table[key_gen].append(idx)\n","    return my_hash_table"],"metadata":{"id":"MN8RD4ND5uAv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Caluclating W_trans_X on training features and creating a hashtable on it\n","x_train = wt_trans_x(X_train_msg_vec_std,hypers)\n","x_train_hast_table = create_hash_table(x_train)"],"metadata":{"id":"8jJNp1ek5xzB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def pred_nearest_neighbor_lsh_labels(train_data,train_features,test_features,x_hash_table,num_of_nbrs):\n","    from collections import Counter\n","    from numpy.linalg import norm\n","\n","    \"\"\"Custom function to caluclate cosine similarities, find the required NNBs labels for the given train and test datasets\"\"\"\n","    #list to store indices of the required NNB's    \n","    label_idx=list()\n","    # a dict to store the counted predicted labels using the indices\n","    label_pred_dict=dict()\n","    # a list to store the finalized predicted label\n","    pred_labels=list()\n","\n","    for fet in test_features.keys():\n","        #key_gen = create_hash_key(fet)\n","        neighbours_x = x_train_hast_table[fet]\n","        neighbours_x_arr = np.array(neighbours_x)\n","        cosine_similarities=[]\n","        for nbr in neighbours_x_arr:\n","            cos_sim=np.dot(train_features[nbr],fet.T).todense().item()/(norm(train_features[nbr].toarray())*norm(fet.T.toarray()))\n","            cosine_similarities.append(cos_sim)\n","        n_11_neighbors=neighbours_x_arr[np.argsort(cosine_similarities)[::-1][:num_of_nbrs]]\n","        label_idx.append(n_11_neighbors)\n","    \n","    for idx,item in enumerate(label_idx):\n","        label_pred_dict[idx]=Counter(list(train_data.iloc[item,0]))\n","\n","    for labels in label_pred_dict.values():\n","        pred_labels.append(max(labels,key=lambda x:labels[x]))\n","        \n","    return pred_labels"],"metadata":{"id":"4BQl2kdz515i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x_test = wt_trans_x(X_test_msg_vec_std,hypers)\n","x_test_hast_table = create_hash_table(x_test)"],"metadata":{"id":"lXsHufvlA_Rc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# predicting labels of test data by providing training data text features\n","my_pred_labels = pred_nearest_neighbor_lsh_labels(X_message_preview,X_train_msg_vec_std,x_test_hast_table,x_train_hast_table,11)"],"metadata":{"id":"H6benoA_5_ri"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[" "],"metadata":{"id":"4BkaUDvLEUBg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"9Ks6ijocyES7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 8.File collections for model deployment"],"metadata":{"id":"_cPEYyzKyGnz"}},{"cell_type":"code","source":["modelo_gbm_pickle_file = open('sms_email_classifier.pkl','wb')\n","pickle.dump(model_xgb,modelo_gbm_pickle_file)\n","modelo_gbm_pickle_file.close()"],"metadata":{"id":"QufHD2yOyNGx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_tfidf_vect_file = open('sms_email_tfidf_vect.pkl','wb')\n","pickle.dump(my_text_vectorizer,model_tfidf_vect_file)\n","model_tfidf_vect_file.close()"],"metadata":{"id":"2UfT6qGTQvou"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["my_df = pd.DataFrame({'message_preview':['Your account is locked due to inactivity reactivate it Bank of america']})"],"metadata":{"id":"BHzn1DKA2TDD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["my_msg = my_df.message_preview"],"metadata":{"id":"hyXeFYxM2ia8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["my_msg"],"metadata":{"id":"3pFquKMESrsn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tf_dep = pickle.load(open('/content/sms_email_tfidf_vect.pkl','rb'))"],"metadata":{"id":"FV-AAtG8Rjb-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["txt_msg = tf_dep.transform(['cannalert grand opening  crestmore smoke house time patients  digitextracted  gram  digitextracted ths bogo carts edibles wax  digitextracted  valley blvd bloomington'])"],"metadata":{"id":"_DlfFs8YUCy2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_dep = pickle.load(open('/content/sms_email_classifier.pkl','rb'))"],"metadata":{"id":"BhZKdcrwURuF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def my_dep_text_transformer(msg):\n","    return tf_dep.transform(msg)"],"metadata":{"id":"vV8Dxpf6J0Rr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def my_dep_predictor(msg_trans):\n","    return model_dep.predict(msg_trans)[0]"],"metadata":{"id":"ywKs47OTKBHr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_dep.predict_proba(txt_msg)[0]"],"metadata":{"id":"4R5C9kuyUefg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["a = my_dep_text_transformer(['wesley create income streams online Great pay Why not begin today! http://onlineinformations.net To unsub text STOP reply HELP for help'])"],"metadata":{"id":"VzWdKhWsKTMq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_dep.predict(a)[0]"],"metadata":{"id":"Yu68yo3OKcQ-"},"execution_count":null,"outputs":[]}]}